{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson1 手書き文字認識をしよう（ニューラルネットワーク入門）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "- Section1 解説\n",
    "  - 1.1 Keras実装プロセス\n",
    "  - 1.2 各モデルLayer\n",
    "  - 1.3 損失関数\n",
    "  - 1.4 評価関数\n",
    "  - 1.5 Functional API\n",
    "  - 1.6 確認問題\n",
    "- Section2 実装①\n",
    "  - 2.1 MNISTによるMLPの復習\n",
    "- Section3 テクニック・発展内容\n",
    "  - 3.1 前処理\n",
    "  - 3.2 勾配に関するテクニック\n",
    "    - 3.2.1 最適化アルゴリズム (optimizer)\n",
    "    - 3.2.2 活性化関数 (activation)\n",
    "    - 3.2.3 初期化 (initializer)\n",
    "  - 3.3 過学習に関するテクニック\n",
    "    - 3.3.1 正則化 (regularization)\n",
    "    - 3.3.2 早期終了 (early stopping)\n",
    "    - 3.3.3 ドロップアウト (dropout)\n",
    "  - 3.4 確認問題\n",
    "- Section4 実装②\n",
    "  - 4.1 Fashion MNIST\n",
    "  - 4.2 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Keras実装プロセス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は早速KerasでMLPの実装方法を見ていきたいと思います。\n",
    "\n",
    "まず、Kerasの雰囲気を感じ取ってもらうため、Kerasで機械学習を行う際に、\n",
    "\n",
    "1. いったいどういった手順を踏むか\n",
    "2. コードはどう書くのか\n",
    "\n",
    "をざっくりと見ていきたいと思います。\n",
    "\n",
    "題材としては、手書き数字画像を入力データ、対応する数字の値を出力データとする教師あり学習（分類）です。\n",
    "\n",
    "参考：https://keras.io/ja/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.0 データの用意\n",
    "まず機械学習を適用するデータを用意しなければ始まりません。\n",
    "\n",
    "ここでは、機械学習で頻繁に用いられる、MNISTの手書き数字データセットを用います。\n",
    "\n",
    "データセットの中身は、\n",
    "\n",
    "* x:手書き数字画像(28×28)\n",
    "* y:正解のラベル（xの画像が表す数字）\n",
    "\n",
    "となっていますが、\n",
    "\n",
    "* (x_train, y_train):モデルの学習用\n",
    "* (x_test, y_test):モデルの評価用\n",
    "\n",
    "と区別してあります。\n",
    "\n",
    "機械学習では汎化性能の向上が至上命題なので、学習用のデータだけでなく評価用のデータが必要になることは前回触れました。\n",
    "\n",
    "MNISTのデータセットも、全てのデータを使用するのではなく、学習用と評価用に予め分割してあるわけです。\n",
    "\n",
    "（分割は事前に行っておく必要があります。評価用のデータまで使用して学習を行うのは、カンニングと変わらなくなってしまいます。）\n",
    "\n",
    "なお、KerasではこのMNISTのデータセットに限らず、機械学習で頻繁に用いられるデータセットがいくつも用意されており、性能評価を手軽に行えます。\n",
    "\n",
    "keras.datasets以下からimportすることで使用できますので、ぜひ使っていきましょう。\n",
    "\n",
    "Kerasから直接使用できるデータセットの一覧はこちら( https://keras.io/ja/datasets/ )です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回扱うMNISTの手書き数字のデータを下記で表示してみましょう。\n",
    "\n",
    "表示する際には、matplotlibを用います。これはPythonでグラフの表示をする際に標準的に用いられるライブラリです。\n",
    "\n",
    "中でもpyplotは最もよく使用されるモジュールで、標準的な描画処理の多くに対応しています。\n",
    "\n",
    "ここでは詳しくは説明しませんが、公式のマニュアルでpyplotに含まれる関数に目を通しておくことをお勧めします。\n",
    "\n",
    "参考：https://matplotlib.org/api/pyplot_api.html\n",
    "\n",
    "なお、jupyer notebook上でmatplotlibの結果を表示するには、`%matplotlib inline`を冒頭で宣言する必要があります。\n",
    "\n",
    "（ちなみに、このような`%`あるいは`%%`から始まるjupyter notebookに対するコマンドはマジックコマンドと呼ばれ、他にも様々なものが存在します。）\n",
    "\n",
    "また、MNISTの画像には、それぞれに対して画像が示す数字が正解のラベルとして与えられています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(9, 15))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# 各MNIST画像の上に（タイトルとして）対応するラベルを表示\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(1, 9, i + 1, xticks=[], yticks=[])\n",
    "    ax.set_title(str(y_train[i]))\n",
    "    ax.imshow(x_train[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これから、このMNISTの各画像が0～9のどの数字であるか分類する事を考えていきます。\n",
    "\n",
    "問題としては、いわゆる10クラス分類の問題です。（分類先のことを**クラス**と呼びます）\n",
    "\n",
    "さて、ここで特に分類タスクの際に気をつけたいことがあります。\n",
    "\n",
    "分類タスクの時の出力データはラベルですが、ラベルは数字としての大小には意味がないということです。\n",
    "\n",
    "というのも、グループの名前として数字を割り振っているだけであるためです。こうした数字を**名義尺度**と呼びます。\n",
    "\n",
    "機械学習のアルゴリズムでは数字の大小に意味があるものとして扱ってしまうため、名義尺度をうまく変換しなければなりません。\n",
    "\n",
    "この名義尺度を変換する表現として使用されるのが、**one-hot表現**と呼ばれるものです。\n",
    "\n",
    "全体で3クラスあるときの各クラスの表現は次の通りです。\n",
    "\n",
    "<ul>\n",
    "    <li>1：$[1,0,0]$</li>\n",
    "    <li>2：$[0,1,0]$</li>\n",
    "    <li>3：$[0,0,1]$</li>\n",
    "</ul>\n",
    "\n",
    "長さ3のベクトルを用いて、各クラスの対応する要素のみ1として表現するということです。\n",
    "\n",
    "一般化すると、全体で$K$クラスある時、$k$番目のクラスに属するとき、\n",
    "\n",
    "$$\\underset{K}{\\underbrace{[0,\\cdots,0,\\overset{k}{\\check{1}},0,\\cdots,0]}}$$\n",
    "\n",
    "と表現するということです。\n",
    "\n",
    "このone-hot表現への変換を行ってくれる関数がKerasにはあります。\n",
    "\n",
    "keras.utils.to_categorical関数がその関数です。さっそくMNISTのデータセットにも適用してみましょう。\n",
    "\n",
    "https://keras.io/ja/utils/#to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 入力画像を行列(28x28)からベクトル(長さ784)に変換\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 名義尺度の値をone-hot表現へ変換\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 モデル構築\n",
    "\n",
    "学習に使用するMLPのモデルを構築します。具体的には、どんなlayer（層）をどこに配置するか、また各layerのユニット数はいくつかを指定していきます。\n",
    "\n",
    "このモデルを構築するための「容器」として機能するのが、keras.models.Sequentialクラスです。\n",
    "\n",
    "この「容器」の中に、Sequential.add関数によってkeras.layersに定義されているlayerクラス（後で詳述）を積み重ねていくことでモデルの構築を行います。\n",
    "\n",
    "layerをSequentialクラスに積み終えたら、最後にSequential.compile関数でモデルの学習処理について指定し、モデル構築は完了です。\n",
    "\n",
    "compile関数では\n",
    "\n",
    "* optimizer（最適化手法）\n",
    "* loss（損失関数）\n",
    "* metrics（評価関数（任意））\n",
    "\n",
    "を指定することになります。（いずれも後で詳述）\n",
    "\n",
    "https://keras.io/ja/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# モデルの「容器」を作成\n",
    "model = Sequential()\n",
    "\n",
    "# 「容器」へ各layer（Dense, Activation）を積み重ねていく（追加した順に配置されるので注意）\n",
    "# 最初のlayerはinput_shapeを指定して、入力するデータの次元を与える必要がある\n",
    "model.add(Dense(units=256, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# モデルの学習方法について指定しておく\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 モデルの学習\n",
    "\n",
    "1.2.1で構築したモデルで実際に学習を行うには、Sequential.fit関数を用います。この関数は固定長のバッチで学習を行います。\n",
    "\n",
    "主な引数は次の通りです。\n",
    "\n",
    "* x：学習に使用する入力データ\n",
    "* y：学習に使用する出力データ\n",
    "* batch_size：学習中のパラメータ更新を1回行うにあたって用いるサンプル数（ミニバッチのサイズ）\n",
    "* epochs：学習のエポック数\n",
    "* verbose：学習のログを出力するか（0:しない、1：バーで出力、2:エポックごとに出力）\n",
    "* validation_split/validation_data：検証用に用いるデータの割合（0～１の実数）、または検証用データそのもの（いずれかのみ指定可能）\n",
    "* shuffle：各エポックごとにデータをシャッフルするか\n",
    "* callbacks：訓練中のモデルの挙動を監視できるcallback関数を指定できます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=1000, epochs=10, verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの評価を行うには、Sequential.evaluate関数を用います。この関数は固定長のバッチごとに損失関数値または評価関数値を出力します。\n",
    "\n",
    "主な引数は次の通りです。\n",
    "\n",
    "* x：評価に使用する入力データ\n",
    "* y：評価に使用する出力データ\n",
    "* batch_size：1回の評価を行うにあたって用いるサンプル数\n",
    "* verbose：評価のログを出力するか（0:しない、1：する(デフォルト)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 モデルによる予測\n",
    "\n",
    "1.2.2で学習させたモデルによって予測を行ってみましょう。Sequential.predict関数によって予測が行えます。\n",
    "\n",
    "主な引数は次の通りです。\n",
    "\n",
    "* x：予測に使用する入力データ\n",
    "* batch_size：まとめて1度に予測を行うサンプル数\n",
    "* verbose：評価のログを出力するか（0:しない(デフォルト)、1：する）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 モデルの可視化\n",
    "\n",
    "1.1.1で作成したモデルは次のようにしてグラフで表現することができます。\n",
    "\n",
    "https://keras.io/ja/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 各モデルLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ここからは、layerクラスについて詳しくみていきましょう。\n",
    "\n",
    "MLPで中心的な存在である、層を表すクラスがlayerクラスです。\n",
    "\n",
    "layerには様々な種類があり、そのそれぞれが独自の機能を持っているので、役割をある程度覚えておきましょう。\n",
    "\n",
    "今回は最もオーソドックスなlayerとして、keras.layers.core以下に定義されている中で使用頻度の高いものを紹介します。\n",
    "\n",
    "https://keras.io/ja/layers/about-keras-layers/\n",
    "\n",
    "https://keras.io/ja/layers/core/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Dense\n",
    "\n",
    "一般的な全結合層を表すレイヤーです。つまり、入力$u\\in\\mathbb{R}^D$に対して、\n",
    "\n",
    "$$v = \\sigma(Wu+b) \\in\\mathbb{R}^{D'}$$\n",
    "\n",
    "を出力します。\n",
    "\n",
    "なお、$W\\in\\mathbb{R}^{D' \\times D}$は重み行列を表し、$b\\in\\mathbb{R}^{D'}$はバイアスを表しています。\n",
    "\n",
    "重み行列とバイアスは学習によって値が決まることに注意しましょう。\n",
    "\n",
    "また$\\sigma(x):\\mathbb{R}^{D'}\\to\\mathbb{R}^{D'}$は**活性化関数**と呼ばれるもので、任意に指定可能です。\n",
    "\n",
    "（実際には最後の出力層の活性化関数については問題の特性から決まることも多いです）\n",
    "\n",
    "一般に活性化関数には非線形関数を指定することで、MLPの性能を向上させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                        kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                        kernel_constraint=None, bias_constraint=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主な引数は\n",
    "\n",
    "* units: 出力ユニット数($N$)\n",
    "* activation: 出力ユニットに適用する活性化関数、Activationレイヤーの説明を参照\n",
    "* use_bias: バイアス$b$を使用するか\n",
    "* kernel_initializer: 重み行列$W$の初期化方法（initializerについては3章で扱います）\n",
    "* bias_initializer: バイアス$b$の初期化方法（initializerについては3章で扱います）\n",
    "\n",
    "です。またshapeの入出力での変化は\n",
    "\n",
    "<ol>(batch_size, ..., input_dim) --> (batch_size, ..., units)</ol>\n",
    "\n",
    "のとおり、一番深いネストの次元がinput_dimからunitsに変わるだけです。\n",
    "\n",
    "1.1.1のモデルの構築で出てきた例を以下に再掲します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=256, input_shape=(784,))) # 次元の変化: 784 -> 256\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=100)) # 次元の変化: 256 -> 100\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=10)) # 次元の変化: 100 -> 10\n",
    "model.add(Activation('softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Activation\n",
    "\n",
    "入力に対して活性化関数を適用したものを出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "keras.layers.core.Activation(activation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数は\n",
    "\n",
    "* activation: 適用する活性化関数\n",
    "\n",
    "のみです。（入力と出力でshapeは変わりません）\n",
    "\n",
    "活性化関数として使用できる関数の一覧はこちら( https://keras.io/ja/activations/ )です。\n",
    "\n",
    "よく使用されるものを以下に示します。\n",
    "\n",
    "* sigmoid: $f(x)=\\dfrac{1}{1+e^{-x}}$\n",
    "* ReLU: $f(x)=\\max(0,x)$\n",
    "* tanh: $f(x)=\\tanh(x)=\\dfrac{e^x-e^{-x}}{e^x+e^{-x}}$\n",
    "* softmax: $f(x)=\\dfrac{\\exp(x_d)}{\\sum_{d'} \\exp(x_{d'})} \\quad (x\\in\\mathbb{R}^D,\\ d=1,2,\\ldots,D)$\n",
    "\n",
    "特にsoftmax関数は出力が規格化されているので、確率として解釈できるため多クラス分類タスクの出力層に使用されることが多いです。\n",
    "\n",
    "（2クラス分類であればsigmoid関数を出力層に使用することも多いです）\n",
    "\n",
    "ここで、活性化関数をプロットしてみましょう。(多変数関数のsoftmaxを除く)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, sigmoid(x), label='sigmoid')\n",
    "ax.plot(x, relu(x), label='ReLU')\n",
    "ax.plot(x, tanh(x), label='tanh')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-1.1, 2)\n",
    "plt.grid(which='major',color='gray',linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1のモデルの構築で出てきた例を以下に再掲します。\n",
    "\n",
    "```py\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=256, input_shape=(784,)))\n",
    "model.add(Activation('relu')) # 活性化関数として relu を選択\n",
    "model.add(Dense(units=100))\n",
    "model.add(Activation('relu')) # 活性化関数として relu を選択\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax')) # 活性化関数として softmax を選択\n",
    "```\n",
    "\n",
    "なお、活性化関数はDenseレイヤーなどで直接指定することも可能で、実際に以下のコードは上記と同じ結果になります。\n",
    "\n",
    "```py\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(256, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Flatten\n",
    "\n",
    "入力をフラット化します。つまり、リストの入れ子になっているデータを1つのリストに展開します。\n",
    "\n",
    "(Ex. [[1,2,3],[4,5,6],[7],[8,9]]->[1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "keras.layers.core.Flatten()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例は次の通りです。（出力shapeはbatch_sizeを除く入力shapeの積）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(3, 32, 32))) # 次元の変化: (3, 32, 32) -> (64, 32, 32)\n",
    "# Conv2Dは未修ですが、ここではその機能は関係ないので気にしなくて結構です\n",
    "\n",
    "model.add(Flatten()) # 次元の変化: (64, 32, 32) -> (65536,) (65536 = 64*32*32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Reshape\n",
    "\n",
    "入力を指定のshapeに変換して出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "keras.layers.core.Reshape(target_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数は\n",
    "* target_shape: 変換先のshapeを表す整数のタプル、ただしサンプルの次元（バッチサイズ）を含まない\n",
    "\n",
    "例は次の通りです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "model = Sequential()\n",
    "model.add(Reshape((3, 4), input_shape=(12,))) # 次元の変化: (12,) -> (3, 4)\n",
    "\n",
    "model.add(Reshape((6, 2))) # 次元の変化: (3, 4) -> (6, 2)\n",
    "\n",
    "# `-1`をしていすると、その次元については推定してくれます(6/2=3)\n",
    "model.add(Reshape((-1, 2, 2))) # 次元の変化: (6, 2) -> (?, 2, 2)=(3, 2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Permute\n",
    "\n",
    "入力の次元を入れ替えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "keras.layers.core.Permute(dims)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数は\n",
    "\n",
    "* dims: 次元の入れ替え方を指定する整数のタプル、サンプルの次元はふくまない1から始まるindexで指定\n",
    "\n",
    "です。（入力と出力でshapeは変わりません）\n",
    "\n",
    "例は次の通りです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 1), input_shape=(10, 64))) # 次元の変化: (10, 64) -> (64, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 RepeatVector\n",
    "\n",
    "入力を指定回数繰り返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "keras.layers.core.RepeatVector(n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引数は\n",
    "\n",
    "* n: 入力の複製回数\n",
    "\n",
    "です。またshapeの入出力での変化は\n",
    "\n",
    "<ol>(num_samples, features) --> (num_samples, n, features)</ol>\n",
    "\n",
    "であり、入力には2階のテンソルのみを受け付けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの学習にあたっては、損失関数の最小化を行うわけでした。そこで、続いて損失関数についてみていきます。\n",
    "\n",
    "https://keras.io/ja/losses/\n",
    "\n",
    "kerasではモデルをコンパイルする際に損失関数を設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数の選択においては、出力値が連続な場合と離散な場合で大きく異なってきます。\n",
    "\n",
    "#### 連続値のとき\n",
    "\n",
    " 主に使用されるのは**平均二乗誤差**です。これは各ミニバッチのデータ数を$N$として、\n",
    "\n",
    " $$E=\\dfrac{1}{N}\\sum_{n=1}^{N}(y_n-t_n)^2$$\n",
    "\n",
    " として表されます。(なお、$y_n, t_n$はそれぞれ入力$x_n$に対するモデルの出力値(y_pred)と出力データ(y_true)を表す)\n",
    "\n",
    " この平均二乗誤差を使用する場合、compile関数の引数として`loss='mean_squared_error'`を指定します。\n",
    "\n",
    "\n",
    "#### 離散値のとき\n",
    "\n",
    "主に使用されるのは、**（多クラス）交差エントロピー**です。2クラス分類の場合は交差エントロピーとして\n",
    "\n",
    "$$E=-\\dfrac{1}{N}\\sum_{n=1}^N \\left[t_n \\ln y_n + (1-t_n) \\ln (1-y_n) \\right]$$\n",
    "\n",
    "を使用し、多クラス分類（Kクラス）の場合は多クラス交差エントロピーとして\n",
    "\n",
    "$$E=-\\dfrac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\ln y_{nk}$$\n",
    "\n",
    "を用います。それぞれcompile関数の引数として`loss='binary_crossentropy'`、`loss='categorical_crossentropy'`を指定することで使用できます。\n",
    "    \n",
    "\n",
    "今回利用したMNISTは0~9の離散値であるため、以下のように多クラス交差エントロピーを利用しています。\n",
    "\n",
    "```py\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 評価関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価関数(metric)はモデルの出力の良し悪しを評価します。\n",
    "\n",
    "損失関数もモデルの良し悪しの指標となるという点では同じですが、損失関数は最適化計算をとおして学習に直接的に影響するのに対して、評価関数は学習には使用されず、あくまでその時点でのモデルの評価指標を出力するのみであるという違いがあります。\n",
    "\n",
    "つまり、compile関数で指定すると、訓練やテストの際に参考情報として評価関数の値が返り値として受け取れるというだけです。\n",
    "\n",
    "評価関数として使用することが多いのは**accuracy(正解率)**です。（正解率＝全体のデータに対して予測値が答えと一致した割合）\n",
    "\n",
    "これはcompile関数の引数として、`metrics=['acc']`を指定することで使用できます。（リストに他の損失関数を含めれば、それらも同時に評価されます）\n",
    "\n",
    "https://keras.io/ja/metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまではkeras.models.Sequentialクラスを用いたモデル構築を説明しました。\n",
    "\n",
    "Sequentialクラスを用いる場合はadd関数を使用して簡単にモデルを構築できますが、途中に分岐や合流があるような複雑なモデルは作成できません。\n",
    "\n",
    "こうしたより複雑なモデルの構築には別の方法が用意されています。それが**Functional API**です。この特徴は\n",
    "\n",
    "* **Inputレイヤー**から構築を始める\n",
    "* 各レイヤーの返り値（テンソル）を次のレイヤーの入力として順々に構築していく\n",
    "* **keras.models.Modelクラス**に入力と出力を指定することでインスタンス化\n",
    "\n",
    "という点です。一度Modelクラスのインスタンスを作ってしまえば、後の学習等はSequentialクラスによる場合と同様です。\n",
    "\n",
    "より詳しくは、実際にFunctional APIが必須になる第4回で扱いますが、すぐにFunctional APIの発展的な利用法をみたいという方は、\n",
    "\n",
    "下記の公式HPのリンクにいくつか記載がありますので参考にしてみてください。\n",
    "\n",
    "https://keras.io/ja/getting-started/functional-api-guide/\n",
    "\n",
    "https://keras.io/ja/models/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 学習データ以外の未知のデータに対するモデルの予測性能を何というか  \n",
    "  ①神通力　②バイアス　③汎化性能　④共起性\n",
    "2. データセット全体を一度に全て使用して学習する方法を何というか  \n",
    "  ①転移学習　②ワンショット学習　③過学習　④バッチ学習　\n",
    "3. 名義尺度のデータをバイナリベクトルによって表現したものを何というか  \n",
    "  ①分散表現　②one-hot表現　③ビット表現　④ユニタリ表現\n",
    "4. モデルの学習に当たって最小化するものは何か  \n",
    "  ①精度　②損失関数　③スコア　④F値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
