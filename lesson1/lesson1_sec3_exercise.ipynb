{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson1 手書き文字認識をしよう（ニューラルネットワーク入門）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "- Section3 テクニック・発展内容\n",
    "  - 3.1 前処理\n",
    "  - 3.2 勾配に関するテクニック\n",
    "    - 3.2.1 最適化アルゴリズム (optimizer)\n",
    "    - 3.2.2 活性化関数 (activation)\n",
    "    - 3.2.3 初期化 (initializer)\n",
    "  - 3.3 過学習に関するテクニック\n",
    "    - 3.3.1 正則化 (regularization)\n",
    "    - 3.3.2 早期終了 (early stopping)\n",
    "    - 3.3.3 ドロップアウト (dropout)\n",
    "  - 3.4 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section3 テクニック・発展内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の高速化や性能向上などのために入力データを変換する前処理を行うことがあります。\n",
    "\n",
    "そのひとつに、データを学習において取り扱いやすいよう、特定の範囲にデータが収まるように変換する**スケーリング**があります。\n",
    "\n",
    "例えば、mnistの入力画像の各ピクセルの値は0~255の範囲の整数値ですが、それぞれ255で割ることで値を0~1の浮動小数点数に収めます。\n",
    "\n",
    "```py\n",
    "x_train, x_test = x_train/255, x_test/255\n",
    "```\n",
    "\n",
    "他にも平均や分散を特定の値にする**正規化**やデータの各要素間の相関を取り除く**白色化**、またデータ全体を考慮した**バッチ正規化**といった前処理もあります。\n",
    "\n",
    "前処理としてはこれらの手法の方が圧倒的に有意義ですが、次のLessonで説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 勾配に関するテクニック\n",
    "\n",
    "MLPにおいてモデルのパラメータを適切に学習することが最大の目的です。\n",
    "\n",
    "つまり、パラメータの更新量が適切に定まることは最も重要と言えるわけです。\n",
    "\n",
    "実はこのパラメータの更新量は**勾配**（＝損失関数のパラメータに関する微分）と呼ばれるものによって決定されますが、\n",
    "\n",
    "この勾配は大きくなりすぎたり、小さくなりすぎたりしやすいもので、適切に取り扱う必要があります。\n",
    "\n",
    "ここでは、そうしたパラメータ更新量や勾配を適切なものとするためのテクニックを3つ（optimizer, activation, initialization）ほど見ていきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 最適化アルゴリズム (optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはパラメータの学習に直接効いてくる、パラメータの更新方法、つまり最適化アルゴリズムから見ていきましょう。\n",
    "\n",
    "最適化アルゴリズムには様々なものがあり、Kerasでも多くのoptimizerの選択肢があります。\n",
    "\n",
    "まずは、最も基本的な**最急降下法**と**確率的勾配降下法(Stochastic Gradient Descent, SGD)**について解説します。\n",
    "\n",
    "その後に、他のoptimizerについて解説します。\n",
    "\n",
    "Kerasの公式HP: https://keras.io/ja/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最急降下法\n",
    "\n",
    "最急降下法は勾配降下法とも呼ばれる、基本的な最適化アルゴリズムで、1階の微分である勾配のみを用いて最適化を行います。\n",
    "\n",
    "パラメータ$\\boldsymbol{w}$の$t$回目の更新を、損失関数を$\\mathrm{E}$として、\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}^{(t+1)} \\leftarrow \\boldsymbol{w}^{(t)} - \\eta\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)}) \\quad \\left(\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)}) = \\left.\\frac{\\partial \\mathrm{E}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}\\right|_{\\boldsymbol{w}=\\boldsymbol{w}^{(t)}}\\right)\n",
    "$$\n",
    "\n",
    "のように行います。$\\eta$は**学習率**と呼ばれるパラメータで、明示的に指定する必要があります。\n",
    "\n",
    "なお、最急降下法では損失関数の計算には常に全訓練データを用い、\n",
    "\n",
    "$$\n",
    "\\mathrm{E}(\\boldsymbol{w}^{(t)})=\\sum_n \\mathrm{E}_n(\\boldsymbol{w}^{(t)})\n",
    "$$\n",
    "\n",
    "として計算します。（$\\mathrm{E}_n$はn番目の訓練データによる損失関数）\n",
    "\n",
    "上の更新式についてイメージを説明すると\n",
    "\n",
    "* $\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})$：パラメータの空間で損失関数が最も増加する方向（これを**勾配**といいます）\n",
    "* $\\eta$：パラメータの空間で損失関数が最も減少する方向$\\left(-\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})\\right)$にどれだけ進むかの比率\n",
    "\n",
    "となっています。\n",
    "\n",
    "この最急降下法はほとんどの最適化手法の基本形になっているので、イメージを抑えておき、各手法の独自性と照らし合わせてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD\n",
    "\n",
    "SGDは最もオーソドックスなMLPのオンライン学習手法です。\n",
    "\n",
    "パラメータの更新を、\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w}^{(t+1)} \\leftarrow \\boldsymbol{w}^{(t)} - \\eta\\nabla\\mathrm{E}^{(t)}(\\boldsymbol{w}^{(t)}) \\quad \\left(\\nabla\\mathrm{E}^{(t)}(\\boldsymbol{w}^{(t)}) = \\left.\\frac{\\partial \\mathrm{E}^{(t)}(\\boldsymbol{w})}{\\partial \\boldsymbol{w}}\\right|_{\\boldsymbol{w}=\\boldsymbol{w}^{(t)}}\\right)\n",
    "$$\n",
    "\n",
    "のように行います。ここで、\n",
    "\n",
    "$$\n",
    "\\mathrm{E}^{(t)}(\\boldsymbol{w}^{(t)})=\\frac{1}{|\\mathfrak{B}^{(t)}|}\\sum_{n\\in\\mathfrak{B}^{(t)}}\\mathrm{E}_n(\\boldsymbol{w}^{(t)})\n",
    "$$\n",
    "\n",
    "です。\n",
    "\n",
    "このように、各更新毎にランダムにミニバッチ（＝訓練データの抜粋、部分集合）$\\mathfrak{B}^{(t)}$を構成し、その平均損失を使用することがSGDの特徴です。\n",
    "\n",
    "Kerasでは以下のクラスを使用します。\n",
    "\n",
    "```py\n",
    "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "```\n",
    "\n",
    "引数は、\n",
    "\n",
    "* lr: 学習率、0以上の実数\n",
    "* momentum: モーメンタム、0以上の実数（前回のパラメータ更新量を反映させる比率（3.2.2で説明））\n",
    "* decay: 更新毎の学習率の減衰率、0以上の実数\n",
    "* nesterov: Nesterov momentumを適用するかどうか（Trueならモーメンタム項の計算を1ステップ先読みして評価します（3.2.2で説明））\n",
    "\n",
    "これはcompile関数の引数として、`optimizer='sgd'`と指定することで使用できます。\n",
    "\n",
    "もしデフォルトでない引数でSGDを使用する場合は`optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)`などと指定してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他の最適化アルゴリズムを考えるにあたって"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDでは学習率は明示的に与える必要がありました。しかし、どのような学習率が最適なのでしょうか？\n",
    "\n",
    "確かに学習率が大きければ、勾配方向への移動距離が大きくなり、より早く学習できそうです。\n",
    "\n",
    "しかし学習が進めば、次第に「最適解」に近づくことが期待されますが、このとき学習率が大きすぎると「通り過ぎる」可能性もあります。\n",
    "\n",
    "（ゴルフのカップ付近でドライバーを振る人はいないわけです）\n",
    "\n",
    "したがって、学習の序盤は大きな学習率で素早く解の付近にたどり着き、次第に学習率を減らして慎重に解に位置を合わせたい訳です。\n",
    "\n",
    "しかし、こうした学習率の調整を手動で行うことは困難です。そこで、学習率の調整が組み込まれた最適化手法が複数提案されています。\n",
    "\n",
    "また、単純なSGDでは、勾配が小さい平坦な場所に来ると更新がほとんど行われなくなってしまうといった問題もありますが、\n",
    "\n",
    "こちらについても同時に対処がなされています。\n",
    "\n",
    "Kerasでは以下で紹介する、より便利な最適化アルゴリズムも、compile時にoptimizerパラメータを変更するだけで簡単に使用可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Momentum\n",
    "\n",
    "前ステップの更新を加味することで、 勾配の変化を滑らかにします。\n",
    "\n",
    "これによって、「通り過ぎる」ことによる行ったり来たりの振動を抑制することができます。\n",
    "\n",
    "また、勾配変化の少ない（パラメータ空間内の）通常の斜面においては、\n",
    "\n",
    "他の勾配変化の大きい斜面と比較して学習率が上昇し、加速的に学習が進むという効果を持っています。\n",
    "\n",
    "Momentumでは前ステップの更新量を加味する割合を'momentum'として指定します。\n",
    "\n",
    "なお、`momentum=0`では通常のSGDと一致することもあり、KerasではSGDクラスのmomentum引数を調整することで使用できます。\n",
    "\n",
    "通常は`momentum=0.9`程度に設定します。\n",
    "\n",
    "```py\n",
    "# Momentumの実装例\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False))\n",
    "```\n",
    "\n",
    "<small>\n",
    "<参考>\n",
    "\n",
    "更新式は次の通りです。($\\boldsymbol{w}^{(t)}$：$t$回目の更新時のパラメータ、$\\eta$：学習率（引数lr）、$\\mu$：前ステップ加味割合（引数momentum）)\n",
    "\n",
    "$$\n",
    "    \\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} + \\Delta\\boldsymbol{w}^{(t)} \\\\\n",
    "    \\Delta\\boldsymbol{w}^{(t)} = \\mu\\Delta\\boldsymbol{w}^{(t-1)} - (1-\\mu)\\eta\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})\n",
    "$$\n",
    "\n",
    "なお、Nesterov's accelerated gradient method (NAG)、つまり'nesterov=True'のときは、\n",
    "\n",
    "$$\n",
    "    \\Delta\\boldsymbol{w}^{(t)} = \\mu\\Delta\\boldsymbol{w}^{(t-1)} - (1-\\mu)\\eta\\nabla \\mathrm{E}(\\boldsymbol{w}^{(t)}+\\mu\\boldsymbol{w}^{(t-1)})\n",
    "$$\n",
    "\n",
    "と更新式が変化し、$t+1$での位置を概算した$\\boldsymbol{w}^{(t)}+\\mu\\boldsymbol{w}^{(t-1)}$での勾配を計算し、より変化を緩やかにします。\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adagrad\n",
    "\n",
    "これまでは全パラメータに対して一様な学習率が設定されていましたが、各方向に対して勾配が異なることを考慮すれば、\n",
    "\n",
    "各パラメータごとに学習率を変化させることができると、より効率的な最適化ができそうです。\n",
    "\n",
    "AdaGradでは、全体の学習率を各方向ごとに過去の勾配の累積で割り引くことで、\n",
    "\n",
    "勾配が大きかった方向の学習率を下げ、小さかった方向の学習率を上げる工夫を導入しています。\n",
    "\n",
    "これによって、たとえ鞍点のようなある方向には勾配が小さいような状況でも、学習が進みやすくなります。\n",
    "\n",
    "Kerasでは,`keras.optimizers.Adagrad`クラスを使用し、通常は学習率として`lr=0.01`程度を用います。\n",
    "\n",
    "なお、AdaGradは学習の初期に勾配が大きいとすぐに更新量が小さくなってしまい、学習がストップしてしまうという欠点があるため、\n",
    "\n",
    "学習率の選択、また重みの初期値の選択は慎重に行う必要があるという欠点をもっていることは気にしておくとよいでしょう。\n",
    "\n",
    "```py\n",
    "# Adagradの実装例\n",
    "from keras.optimizers import Adagrad\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adagrad(lr=0.01, epsilon=1e-08, decay=0.0))\n",
    "```\n",
    "\n",
    "<small>\n",
    "<参考>\n",
    "\n",
    "更新式は次の通りです。（$\\Delta\\boldsymbol{w}^{(t)}_i$：$t$回目のパラメータ更新量＠第$i$成分、$\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i$：$t$回目のパラメータによる勾配＠第$i$成分、$\\eta$：学習率（引数lr））\n",
    "\n",
    "$$\n",
    "    \\Delta\\boldsymbol{w}^{(t)}_i = -\\frac{\\eta}{\\sqrt{\\sum^{t}_{s=1}\\left(\\nabla\\mathrm{E}(\\boldsymbol{w}^{(s)})_i\\right)^2 + \\varepsilon}}\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i\n",
    "$$\n",
    "\n",
    "なお、$\\varepsilon$は計算機上での0割りを回避するためのもので、ごく小さい値（`epsilon=10^-8`）を指定します。\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSprop\n",
    "\n",
    "AdaGradでは勾配の蓄積が大きくなり、更新量が小さくなると二度と大きくなることがないという欠点がありました。\n",
    "\n",
    "RMSpropでは、この点に対処するため、勾配の情報が指数的な減衰によって次第に忘却されるように更新式を変更したことが特徴的になっています。\n",
    "\n",
    "（＝勾配の2乗の指数移動平均を使用）\n",
    "\n",
    "Kerasでは、`keras.optimizers.RMSprop`クラスを用います。勾配の指数移動平均を制御するパラメータとして`rho`が新たに指定できる点が特徴的です。\n",
    "\n",
    "`rho`はどれだけ過去の勾配を重視するかを表し、通常は`rho=0.9`程度とすることが多いです。\n",
    "\n",
    "```py\n",
    "# RMSPropの実装例\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "```\n",
    "\n",
    "<small>\n",
    "<参考>\n",
    "\n",
    "更新式は次の通りです。（$\\Delta\\boldsymbol{w}^{(t)}_i$、$\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i$、$\\eta$：AdaGradと同じ、$\\rho$：勾配情報の減衰率（引数rho））\n",
    "\n",
    "$$\n",
    "    v_i^{(t)} = \\rho v_i^{(t-1)} + (1-\\rho)(\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i)^2 \\quad (v_i^{(0)}=0)\\\\\n",
    "    \\Delta\\boldsymbol{w}^{(t)}_i=-\\frac{\\eta}{\\sqrt{v_i^{(t)}+\\varepsilon}}\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i\n",
    "$$\n",
    "\n",
    "学習率の割り引き方が、2乗和のルートから勾配の2乗の指数移動平均のルートに変化している点にも注意してください。\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaDelta\n",
    "\n",
    "RMSpropによって学習率が不可逆的に悪化することを防ぐことができましたが、AdaGradの全体の学習率に鋭敏であるという性質はそのままです。\n",
    "\n",
    "この全体の学習率への鋭敏性、つまり問題設定毎に適切な学習率が変化してしまうという問題は、\n",
    "\n",
    "実は更新量と勾配の次元の不一致を学習率で調整していることによるものです。（ここでの次元は物理的な次元のことで、いわゆる単位に相当するものです）\n",
    "\n",
    "そこで、AdaDeltaではそうした次元の不一致を加味して自動的に適切な学習率が設定されるようにしています。\n",
    "\n",
    "具体的には、勾配の2乗の指数移動平均に加えて、更新量の2乗の指数移動平均をもちい、両者の比を学習率として設定しています。\n",
    "\n",
    "（なぜこれで次元の不一致に対処可能かは詳しく扱いませんが、Newton法が次元に対してロバストである＋Hessian逆行列の近似を利用して導出されます）\n",
    "\n",
    "Kerasでは、`keras.optimizers.Adadelta`クラスを用います。RMSpropと同様に、更新量と勾配の指数移動平均を制御するパラメータ`rho`を設定できます。\n",
    "\n",
    "通常`rho=0.95`とすることが推奨されています。\n",
    "\n",
    "なお、Kerasの実装では一応学習率`lr`を設定できるようになっていますが、AdaDeltaの提案論文では学習率は自動的に決定されるものとしている上、\n",
    "\n",
    "Kerasの公式HPでも`lr`はデフォルトのままとすることを推奨しているため、学習率の設定は基本的に不要です。\n",
    "\n",
    "```py\n",
    "# Adadeltaの実装例\n",
    "from keras.optimizers import Adadelta\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0))\n",
    "```\n",
    "\n",
    "<small>\n",
    "<参考>\n",
    "\n",
    "更新式は次の通りです。（$\\Delta\\boldsymbol{w}^{(t)}_i$、$\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i$：AdaGradと同じ、$\\rho$：勾配・更新量情報の減衰率（引数rho））\n",
    "\n",
    "$$\n",
    "    u_i^{(t)} = \\rho u_i^{(t-1)} + (1-\\rho)(\\Delta\\boldsymbol{w}^{(t)}_i)^2 \\quad (u_i^{(0)}=0)\\\\\n",
    "    v_i^{(t)} = \\rho v_i^{(t-1)} + (1-\\rho)(\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i)^2 \\quad (v_i^{(0)}=0)\\\\\n",
    "    \\Delta\\boldsymbol{w}^{(t)}_i=-\\frac{\\sqrt{u_i^{(t)}+\\varepsilon}}{\\sqrt{v_i^{(t)}+\\varepsilon}}\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i\n",
    "$$\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam\n",
    "\n",
    "AdaDeltaとは異なるRMSpropの改良法としてAdamが挙げられます。\n",
    "\n",
    "Adamでは、各方向への勾配の2乗に加えて勾配自身も、指数移動平均による推定値に置き換えています。\n",
    "\n",
    "これにより、ある種Momentumと似た効果が期待できます。\n",
    "\n",
    "Kerasでは、`keras.optimizers.Adam`クラスを使用します。\n",
    "\n",
    "パラメータとしては、勾配、勾配の2乗それぞれの指数移動平均を制御するパラメータとして`beta_1,beta_2`が新たに指定可能です。\n",
    "\n",
    "といっても、ほとんどの場合はデフォルトのパラメータが推奨され、実際に使用されています。\n",
    "\n",
    "```py\n",
    "# Adamの実装例\n",
    "from keras.optimizers import Adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    ")\n",
    "```\n",
    "\n",
    "<small>\n",
    "<参考>\n",
    "\n",
    "更新式は次の通りです。（$\\Delta\\boldsymbol{w}^{(t)}_i$、$\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i$、$\\eta$：RMSpropと同じ、$\\rho_1,\\rho_2$：勾配情報の減衰率（引数beta_1,beta_2））\n",
    "\n",
    "$$\n",
    "    m_i^{(t)} = \\rho_1 m_i^{(t-1)} + (1-\\rho_1)\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i \\quad (m_i^{(0)}=0)\\\\\n",
    "    v_i^{(t)} = \\rho_2 v_i^{(t-1)} + (1-\\rho_2)(\\nabla\\mathrm{E}(\\boldsymbol{w}^{(t)})_i)^2 \\quad (v_i^{(0)}=0)\\\\\n",
    "    \\hat{m}_i^{(t)} = \\frac{m_i^{(t)}}{1-(\\rho_1)^t}\\\\\n",
    "    \\hat{v}_i^{(t)} = \\frac{v_i^{(t)}}{1-(\\rho_2)^t}\\\\\n",
    "    \\Delta\\boldsymbol{w}^{(t)}_i=-\\frac{\\eta}{\\sqrt{\\hat{v}_i^{(t)}+\\varepsilon}}\\hat{m}_i^{(t)}\n",
    "$$\n",
    "\n",
    "なお、$m_i^{(t)},v_i^{(t)}$を$\\hat{m}_i^{(t)},\\hat{v}_i^{(t)}$に変換しているのは、\n",
    "\n",
    "各々勾配の1次モーメントと2次モーメントの推定量としてみた時、バイアスを持ってしまうため、その分を補正する役割です。\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、鞍点における各種最適化手法の挙動を視覚的に把握しておきましょう。（Adamは除く、NAGはNestorovのMomentum）\n",
    "\n",
    "SGD以外の手法は、勾配が小さくなってしまう鞍点においてもうまく機能していることが見て取れるかと思います。\n",
    "\n",
    "![optimizer](figures/optimizer.gif)\n",
    "出典：http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 活性化関数 (activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いての勾配に関するテクニックは、活性化関数の選択です。 \n",
    "\n",
    "先の最適化手法で見た通り、勾配がパラメータ更新で中心的な役割を果たし、その大きさが適切に保たれることは極めて重要です。\n",
    "\n",
    "しかし、一方でこの勾配の大きさについて**勾配消失問題**という課題が知られており、活性化関数の工夫はその対処法の1つとなっています。\n",
    "\n",
    "勾配消失問題は、文字通り、勾配（＝損失関数のパラメータ微分）が数値計算上極めて小さくなってしまい、学習が進まなくなってしまうという問題です。\n",
    "\n",
    "勾配消失問題はなぜ発生するのでしょうか。それはニューラルネットワークの勾配の求め方である**誤差逆伝播法**を知ることで理解できます。\n",
    "\n",
    "数学的詳細は省略しますが、誤差逆伝播法では、**（ある層の勾配）＝（1層前の勾配）×（2層前の勾配）×・・・×（出力層の勾配）**と積の形で勾配を求めるため、途中の勾配が小さいと入力層付近の勾配はどんどん0に近づいていってしまうわけです。\n",
    "\n",
    "したがって、勾配消失問題は特に多層な場合には致命的になります。\n",
    "\n",
    "![bp](figures/backprop.png)\n",
    "出典：R. Abedini , M. Esfandyari , A. Nezhadmoghadam and B. Rahmanian, “The Prediction of Undersaturated Crude Oil Viscosity: An Artificial Neural Network and Fuzzy Model Approach,” Petroleum Science and Technology, vol. 30, no. 19, pp. 2008–2021, 2012. ( http://www.tandfonline.com/doi/abs/10.1080/10916466.2010.512892 )\n",
    "\n",
    "とはいえ、勾配を求めるにあたって、差分による数値微分などの他の手法を利用することは計算量的に現実的ではありません。\n",
    "\n",
    "そこで、この勾配消失問題への対処の1つとして, 活性化関数の微分の値がなるべく小さくならないようなものを選びたいというわけです。\n",
    "\n",
    "例えば、先程活性化関数として扱った中で、sigmoid, tanh, ReLU関数の微分はそれぞれ以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))**2\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの三種の微分をグラフで表示することで、勾配（微分）がどのように変化するか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# sigmoid\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, deriv_sigmoid(x), label='sigmoid deriv')\n",
    "\n",
    "# tanh\n",
    "ax.plot(x, deriv_tanh(x), label='tanh deriv')\n",
    "\n",
    "# relu\n",
    "ax.plot(x, deriv_relu(x), label='ReLU deriv')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図より, sigmoidよりもtanh、reluのほうがより大きな値をとり、勾配消失しにくいことがわかります。\n",
    "\n",
    "最近の論文でもreluもしくはその派生形を用いているものが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 初期化 (initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配に関するテクニックの3つめはパラメータの初期化についてです。\n",
    "\n",
    "各層のパラメータは0を中心とした乱数で初期化しますが、大きすぎる値で初期化すれば学習の初期段階での勾配が過大になり、\n",
    "\n",
    "逆に小さすぎる値だと勾配自体も過小になってしまい、いずれにしても学習はうまく進みません。\n",
    "\n",
    "そこで、初期化にあたっては、その値のスケール（分散）を適切に設定する必要があります。\n",
    "\n",
    "このパラメータの初期化にあたって比較的頻繁に用いられる手法として、LeCunによる手法、Glorotによる手法、Heによる手法が挙げられます。\n",
    "\n",
    "特にGlorotの初期化法は活性化関数が全て線形な場合の解析結果であり、中央付近が線形とみなせるsigmoid関数やtanh関数に適していると言えます。\n",
    "\n",
    "また、Heの初期化法は活性化関数がReLUであるときに適しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LeCunの初期化\n",
    "\n",
    "各層の入力次元を$n_{in}$として, 次のように初期化します.\n",
    "$$\n",
    "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{3}{n_{\\textrm{in}}}}, \\sqrt{\\frac{3}{n_{\\textrm{in}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\frac{1}{\\sqrt{n_{\\textrm{in}}}}\\right)\n",
    "$$\n",
    "\n",
    "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
    "\n",
    "Kerasでは、それぞれ`keras.initializers.lucun_uniform`、`keras.initializers.lucun_normal`として定義されていますが、\n",
    "\n",
    "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
    "\n",
    "```py\n",
    "# LuCun's initializationの実装例\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='lucun_uniform'))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='lucun_normal'))\n",
    "```\n",
    "\n",
    "参考：\n",
    "https://keras.io/ja/initializers/#lecun_normal\n",
    "https://keras.io/ja/initializers/#lecun_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Glorotの初期化（Xavierの初期化）\n",
    "\n",
    "各層の入力次元を$n_{\\textrm{in}}$, 出力次元を$n_{\\textrm{out}}$として, 次のように初期化します.\n",
    "$$\n",
    "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}+n_{\\textrm{out}}}}\\right)\n",
    "$$\n",
    "\n",
    "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
    "\n",
    "Kerasでは、それぞれ`keras.initializers.glorot_uniform`、`keras.initializers.glorot_normal`として定義されていますが、\n",
    "\n",
    "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
    "\n",
    "```py\n",
    "# Glorot's initializationの実装例\n",
    "model.add(Dense(128, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(128, activation='sigmoid', kernel_initializer='glorot_normal'))\n",
    "```\n",
    "\n",
    "参考：\n",
    "https://keras.io/ja/initializers/#glorot_normal\n",
    "https://keras.io/ja/initializers/#glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Heの初期化\n",
    "\n",
    "各層の入力次元を$n_{\\textrm{in}}$として, 次のように初期化します.\n",
    "$$\n",
    "    \\theta \\sim \\mathcal{U}\\left(-\\sqrt{\\frac{6}{n_{\\textrm{in}}}}, \\sqrt{\\frac{6}{n_{\\textrm{in}}}}\\right) \\quad \\textrm{or} \\quad \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\textrm{in}}}}\\right)\n",
    "$$\n",
    "\n",
    "なお、$\\mathcal{U}$は一様分布、$\\mathcal{N}$は正規分布を表します。\n",
    "\n",
    "Kerasでは、それぞれ`keras.initializers.he_uniform`、`keras.initializers.he_normal`として定義されていますが、\n",
    "\n",
    "以下のようにlayerの引数としてキーワードで指定することも可能です。\n",
    "\n",
    "```py\n",
    "# He's initializationの実装例\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_normal'))\n",
    "```\n",
    "\n",
    "参考：\n",
    "https://keras.io/ja/initializers/#he_normal\n",
    "https://keras.io/ja/initializers/#he_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 過学習に対するテクニック\n",
    "\n",
    "ここでMLPの教師有り学習における真の目標を振り返っておきましょう。それは、新しく未知のデータが来た時に正確に予測することです。\n",
    "\n",
    "一方で、我々がこれまで扱ってきたものは、学習＝既知のデータに対して正確に予測することです。\n",
    "\n",
    "この違いは実は大きな問題となります。既知のデータへの予測精度を十分高めたからと言って、未知のデータへの予測精度が上がるとは限りません。\n",
    "\n",
    "具体的には、学習がある程度以上進むと、次第に既知データが持つ、それ自身には意味がないような統計的なばらつきまで学習してしまい、\n",
    "\n",
    "未知データへの予測精度が落ちるという現象、すなわち**過学習**が発生するわけです。\n",
    "\n",
    "したがって、単に目的関数を訓練データに対して最小化すればよいという問題ではなくなってきます。\n",
    "\n",
    "こうした過学習へのアプローチを3つほど取り上げたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 正則化 (regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習が発生する一つの理由は、MLPのモデルは特に多くのパラメータ・自由度を持つために、\n",
    "\n",
    "訓練データに対して、その本質的な部分以上に統計的ばらつきまで含めて完全にフィットしようとしてしまうことにあります。 \n",
    "\n",
    "そこで過学習を回避するには、学習過程でいくつかのパラメータが自動的に機能しなくなると良いわけですが、これを実現するのが**正則化**です。\n",
    "\n",
    "具体的には、損失関数にパラメータの大きさに対するペナルティ項（正則化項）を含めます。\n",
    "\n",
    "これにより, パラメータを自由に動けないように制限し、なるべく少ないパラメータでデータにフィットするようにできます。\n",
    "\n",
    "（実際の挙動としては、パラメータの値が0に近づくようにするということです。0であればパラメータは実質的に機能しません。）\n",
    "\n",
    "なお、正則化には様々な種類が存在し、主に次のL2,L1正則化またそれらを組み合わせたElasticNetが用いられます。\n",
    "\n",
    "参考：https://keras.io/ja/regularizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L2正則化\n",
    "\n",
    "L2正則化では、全パラメータの2乗和を正則化項として損失関数に加えます。\n",
    "\n",
    "L2正則化では、パラメータを完全に0にすることは少ないものの、パラメータを滑らかにすることで予測精度のより良いモデルを構築します。\n",
    "\n",
    "Kerasでは`keras.regularizers.l2`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
    "\n",
    "なお、`keras.regularizers.l2`は引数として、正則化項に掛かる係数を指定できます。\n",
    "\n",
    "```py\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "<small>\n",
    "\n",
    "<参考>\n",
    "$\\Theta$：パラメータ、$\\lambda$：係数（＝引数）\n",
    "$$\n",
    "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}w_i^2\n",
    "$$\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### L1正則化\n",
    "\n",
    "L1正則化では、全パラメータの絶対値の和を正則化項として損失関数に加えます。\n",
    "\n",
    "L1正則化ではL2正則化よりもパラメータが0になりやすいという特徴（**スパース性**）があります。\n",
    "\n",
    "Kerasでは`keras.regularizers.l1`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
    "\n",
    "なお、`keras.regularizers.l1`は引数として、正則化項に掛かる係数を指定できます。\n",
    "\n",
    "```py\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l1(0.01))\n",
    "```\n",
    "<small>\n",
    "\n",
    "<参考>\n",
    "$\\boldsymbol{w}$：パラメータ、$\\lambda$：係数（＝引数）\n",
    "$$\n",
    "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}|w_i|\n",
    "$$\n",
    "\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ElasticNet\n",
    "\n",
    "L1正則化とL2正則化の組み合わせです。\n",
    "\n",
    "Kerasでは`keras.regularizers.l1_l2`として定義されていますが、実際には各layerにregularizerとして引数で指定して用います。\n",
    "\n",
    "なお、`keras.regularizers.l1_l2`は引数として、各々の正則化項に掛かる係数を指定できます。\n",
    "\n",
    "```py\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "model.add(Dense(128, kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "```\n",
    "<small>\n",
    "\n",
    "<参考>\n",
    "$\\boldsymbol{w}$：パラメータ、$\\lambda$：係数、$\\alpha$：L1正則化とL2正則化の割合\n",
    "$$\n",
    "    \\mathrm{E}(\\boldsymbol{w}) = \\sum^{N}_{n=1}\\sum^{K}_{k=1} t^{(n)}_{k} \\ln y^{(n)}_k + \\lambda \\sum_{i}[\\alpha|w_i|+(1-\\alpha)w_i^2]\n",
    "$$\n",
    "よくある定義式としては上の通りですが、Kerasの実装との対応は、\n",
    "$$\n",
    "    l1=\\lambda\\alpha, \\quad l2=\\lambda(1-\\alpha) \\Leftrightarrow \\lambda=l1+l2, \\quad \\alpha = \\frac{l1}{l1+l2}\n",
    "$$\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に学習したモデルのコストをチェックする際は、正則化項を含めない値が他のモデルに比べて減っているかを見る必要があります。\n",
    "\n",
    "というのも、正則化項はあくまで学習の都合上導入されたもので、予測の意味では、正則化項を含めない目的関数で評価すべきであるためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 早期終了 (early stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下図の通り、過学習はある一定以上学習が進んでから発生します。\n",
    "\n",
    "またこのとき、学習データによる評価関数値が改善しても、検証データによる評価関数値はさほど改善しないか悪化します。\n",
    "\n",
    "![es](figures/earlystopping.png)\n",
    "\n",
    "出典：https://deeplearning4j.org/earlystopping\n",
    "\n",
    "つまり、過学習に突入してからの学習は、未知のデータに対する予測力に寄与しない、（統計的なばらつき等の）本質的でない部分の学習なわけです。\n",
    "\n",
    "そこで、いっそのこと早々に学習を止めてしまうことで過学習を回避する手もあります。それが**早期終了(early stopping)**です。\n",
    "\n",
    "といっても、具体的にいつ学習を止めるかが重要です。\n",
    "\n",
    "先述の通り、訓練データに対する誤差は最適化によってほとんどの場合低下しますが、検証データの誤差は過学習時には上昇します。\n",
    "\n",
    "そこで、検証データの誤差が大きくなってきた（或いは評価関数値が下がってきた）ところで学習をストップさせます。\n",
    "\n",
    "`Keras`ではcallbackという機能を使って各エポック毎のモデルにパラメータを保存し、検証データのコストが大きくなったら前のエポックのパラメータを使用するようにします。\n",
    "\n",
    "`model.fit`の引数に以下のように設定します。\n",
    "\n",
    "```py\n",
    "model.fit(x=x_train, y=y_train, ...,\n",
    "    callbacks=keras.callbacks.EarlyStopping(patience=0, verbose=1))\n",
    "```\n",
    "\n",
    "参考：https://keras.io/ja/callbacks/#earlystopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 ドロップアウト (dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習で余計に学習している部分に着目すると、それは統計的なばらつきと言えるような部分でした。\n",
    "\n",
    "こうした確率的なばらつきは、一般に大量に足し合わせると相殺されます。\n",
    "\n",
    "そこで、訓練データセットから部分訓練データセットを大量に作成し、各モデルの予測結果を平均するアンサンブルという手法が用いられることがあります。\n",
    "\n",
    "このアンサンブル法は大変魅力的な手法なのですが、とてつもない計算量を要するためそのまま用いることは難しいものでした。\n",
    "\n",
    "そこで出てきたものが、**ドロップアウト (dropout)**と呼ばれる手法で、これは近似的にアンサンブル法を実現するものになっています。\n",
    "\n",
    "具体的には、ドロップアウトは入力の一部をランダムに0にして出力するlayerの一種です。要するに一部のユニットを取り除いた状況を再現します。\n",
    "\n",
    "このユニットの除去を確率的に行い、一部のユニットが除去された部分ネットワークに対して学習することを繰り返すことで、\n",
    "\n",
    "多数のモデルを同時に訓練することと同じ効果を再現しているわけです。\n",
    "\n",
    "Kerasでは、`keras.layers.core.Dropout`クラスを用いて実装できます。\n",
    "\n",
    "```py\n",
    "keras.layers.core.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "\n",
    "主な引数は、\n",
    "\n",
    "* rate: 入力を0にする確率、0～1の実数値\n",
    "* seed: 乱数のシード値\n",
    "\n",
    "です。（入力と出力でshapeは変わりません）\n",
    "\n",
    "参考：https://keras.io/ja/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 確認問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 損失関数のパラメータ微分のことを何というか  \n",
    "  ①スケーリング　②勾配　③学習率　④SGD\n",
    "2. 活性化関数の選択により対処可能な問題を何というか  \n",
    "  ①勾配消失問題　②次元の呪い　③固有値問題　④バイアス―バリアンストレードオフ\n",
    "3. 損失関数にペナルティ項を加え、パラメータの自由度を下げる手法を何というか  \n",
    "  ①モーメンタム　②部分空間法　③正則化　④誤差逆伝播法\n",
    "4. 過学習への対処としてふさわしくないものはどれか  \n",
    "  ①ドロップアウト　②スケーリング　③ElasticNet　④早期終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
