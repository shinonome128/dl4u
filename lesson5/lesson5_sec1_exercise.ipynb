{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson5 画像からキャプションを生成してみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- Section1 解説\n",
    "    - 1.1 キャプション生成とは?\n",
    "    - 1.2 条件付き言語モデルとしての位置づけ\n",
    "    - 1.3 データセット\n",
    "        - MS-COCO\n",
    "    - 1.4 ネットワーク構成\n",
    "        - Encoder (CNN)\n",
    "        - Decoder (RNN)\n",
    "- Section 2 実装①\n",
    "    - 2.1 MS-COCOデータでキャプション生成\n",
    "- Section 3 テクニック・発展的内容\n",
    "    - 3.1 Attention\n",
    "    - 3.2 Beam Search\n",
    "- Section 4 実装②\n",
    "    - CNN-LSTM with Attention\n",
    "- Section 5 ケーススタディ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 キャプション生成とは\n",
    "**キャプション生成** とは、画像を入力としてその画像の説明文（キャプション）を出力するタスクです。\n",
    "\n",
    "以下イメージです。\n",
    "\n",
    "<img src=\"./figures/cap_gen_image.png\" width=\"500m\">\n",
    "出典: O. Vinyals et al. Show and Tell: A Neural Image Caption Generator. CVPR 2015\n",
    "\n",
    "研究の意義としては、例えば視覚障害のある人にWeb上にある画像の内容を理解させるために自動で説明文を生成する、といったことなどが考えられます。\n",
    "\n",
    "基本的なネットワーク構成としては、画像の特徴量をCNNにより抽出し、その特徴量をもとにRNNで説明文を生成していく形になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 前Lesson（翻訳モデル）との関係性\n",
    "\n",
    "前Lessonで学んだ翻訳モデルとの関係性を考えてみます。\n",
    "\n",
    "翻訳モデルでは、Encoder (RNN) で元言語文の特徴を抽出し、その抽出した特徴をもとにDecoder (RNN) でターゲット言語の文を生成していました。\n",
    "\n",
    "キャプション生成のモデルでも基本的な構造は同じです。CNNで画像の特徴を抽出し、その特徴をもとに説明文を生成します。\n",
    "\n",
    "翻訳タスクもキャプション生成タスクも、同じ言語生成という広いくくりのタスクとして俯瞰することができます。\n",
    "\n",
    "これらの違いは、どのようにDecoder側の言語モデルの初期状態 (具体的にはRNNであれば最初の隠れ層$h_0$) を条件付けるかという部分です。\n",
    "\n",
    "こういったように、問題設定をうまく工夫することで、一口に言語生成といっても要約・対話などさまざまなタスクに対処できるようになります。\n",
    "\n",
    "他にも次の表のような例があります。\n",
    "\n",
    "<img src=\"./figures/clm.png\" width=\"600mm\">\n",
    "\n",
    "出典: C. Dyer. \"Conditional Language Modeling\" in Deep Natural Language Processing 2017 at Oxford. https://github.com/oxford-cs-deepnlp-2017/lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題設定が変わったからと言って大幅にモデルが変わってしまうということもありません。\n",
    "\n",
    "具体的には、キャプション生成のモデルでは入力情報が画像となるので、EncoderをCNNに変更するだけで良いわけです。\n",
    "\n",
    "（翻訳モデルでは入力情報が文でしたので、系列に特化したRNNを特徴抽出に用いていました）\n",
    "\n",
    "後述するAttentionの利用についても、同じように元情報の特徴量に対して適用するという構図は同じです。\n",
    "\n",
    "翻訳では元文の各隠れ層状態を参考にAttentionを適用していましたが、キャプション生成では元画像のCNNの出力の各ピクセルに対して適用していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 データセット\n",
    "\n",
    "キャプション生成のタスクにおいてもいくつか代表的なデータセットがありますが、今回はそのうちの１つのMS-COCOとよばれるものを使います。\n",
    "\n",
    "MS-COCOでは画像に対してキャプションや画像内の物体のラベル等が付いています。\n",
    "\n",
    "日本語に翻訳された物もあります。(https://github.com/STAIR-Lab-CIT/STAIR-captions)\n",
    "\n",
    "今回の演習ではMS-COCOのキャプションのデータの一部 (学習用: 50,000ペア、検証用: 1,000ペア) を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずMSCOCOのデータを`download/`以下にdownloadします。(時間がかかるので注意してください)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh download_mscoco.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に画像及びキャプションデータに前処理を施します。(画像サイズの統一、キャプションのtokenize等)\n",
    "前処理後のデータは`data/`以下に保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python preprocess_mscoco.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中身を少し見てみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_y = list(open('./data/y_train.txt'))[:2]\n",
    "data_x = np.load('./data/x_train.npy')[:2]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, (x, y) in enumerate(zip(data_x, data_y)):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.imshow(x)\n",
    "    plt.title(y)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ネットワーク構成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 概要\n",
    "今回のキャプション生成では、上述の通り、画像の特徴量の抽出にCNN、説明文の生成にRNNを用います。\n",
    "\n",
    "![cap_gen_image](./figures/cap_gen_image.png)\n",
    "[Show and Tell: A Neural Image Caption Generator](https://arxiv.org/pdf/1411.4555.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Encoder\n",
    "CNN (VGGNet) を使用します。\n",
    "ここで、事前に大規模なデータセット (ImageNet) で学習されたモデルの重みを利用 (転移) します。\n",
    "これは`Keras.applications`から読み込むことができます。\n",
    "\n",
    "VGG16の引数としては、`weights`、`include_top`、`input_tensor`の3つがあります。\n",
    "- weights: 重みの種類を指定します。ここでは、ImageNetで学習した重みを使用します。重みを指定せずに自分で重みを学習することも出来ますが、学習には大量の画像が必要となるなどコストがかかります。\n",
    "- include_top: VGG16の出力層を含むかどうかを決定します。ここではあくまでも特徴抽出器として使いたいので含みません。\n",
    "- input_tensor: 入力画像のサイズを指定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、今回のモデルでの学習時にはこのCNNの重みの更新は行いません。\n",
    "学習を容易にする (学習パラメータの数を減らす) ためです。\n",
    "\n",
    "具体的な実装はSection 2にて行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "x = Input(shape=(224,224,3))\n",
    "model = VGG16(\n",
    "    weights='imagenet', \n",
    "    include_top=False,\n",
    "    input_tensor=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワーク構成の確認は`.summary()`でできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Decoder\n",
    "ここでは翻訳モデルと同様にLSTMを用います。\n",
    "LSTMの隠れ層とメモリセルの初期状態$h_0$、$c_0$はEncoder(CNN)の出力を元に決定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Checkクイズ\n",
    "問1. キャプション生成におけるネットワークの流れとして適切なものを選択肢から一つ選びなさい。\n",
    "\n",
    "  ① Encoder(CNN)の出力は、Decoder(RNN)に初期状態$h_0$として入力される\n",
    "  \n",
    "  ② Encoder(CNN)の出力は、Decoder(RNN)に最初の単語の埋め込みベクトルとして入力される"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
