{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson7 ニューラルネットでゲームを攻略するAIをつくろう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Section 1 解説\n",
    "    - 1.1 強化学習とは\n",
    "    - 1.2 強化学習の定式化\n",
    "    - 1.3 Temporal Difference (TD) 学習\n",
    "    - 1.4 Q学習\n",
    "    - 1.5 Deep Q学習\n",
    "    - 1.6 Checkクイズ\n",
    "- Section 2 実装①\n",
    "    - 2.1 OpenAI Gymについて\n",
    "    - 2.2 Colab上でのレンダリングについて\n",
    "    - 2.3 DQNでCartPole\n",
    "- Section 3 テクニック・発展的内容\n",
    "    - 3.1 Prioritized Experience Replay\n",
    "    - 3.2 Double DQN\n",
    "    - 3.3 Dueling Network\n",
    "    - 3.4 Checkクイズ\n",
    "- Section 4 実装②\n",
    "    - 4.1 DQN + テクニックでBreakout\n",
    "- Section 5 ケーススタディ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkクイズの解答\n",
    "問1: ①,  問2: ②,  問3: ①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 実装①"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 OpenAI Gymについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI GymとはOpenAIが開発している強化学習研究・開発のためのプラットフォームです。 CartpoleやAtariのゲームなどが簡単に利用できるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使う流れとしては、\n",
    "\n",
    "1. `gym.make('ゲームの名前')`: 環境を構築\n",
    "2. `env.reset()`: エピソードを開始\n",
    "3. `next_state, reward, terminal, _ = env.step(action)`: 行動を環境に渡し、 新しい状態, 報酬, episodeが終わったかどうかのフラグを受け取る。\n",
    "\n",
    "となります。 また、 `env.render()`を実行することでゲームの途中の経過を確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make('Breakout-v0') # ゲーム環境の構築\n",
    "\n",
    "for episode in range(1):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    # Plot (説明の便宜上のもの)\n",
    "    img = plt.imshow(state)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    while not terminal:\n",
    "#         env.render() # ゲーム画面の出力\n",
    "\n",
    "        action = env.action_space.sample() # とりあえず行動をランダムに選択\n",
    "        next_state, reward, terminal, _ = env.step(action) # 行動を環境に渡し、 新しい状態, 報酬, エピソードが終わったかのフラグを受け取る\n",
    "        \n",
    "        # Plot (説明の便宜上のもの)\n",
    "        img.set_data(next_state)\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Colab上でのレンダリングについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ローカルPCなどでは`env.render()`を実行することにより新しいwindowが出てきて特に苦労なく出力を確認することができますが、   \n",
    "Google Colaboratory（以下Colab）上で出力を確認する場合には`env.render()`が使えないため、以下を実行する必要があります。  \n",
    "（※非公式の方法のため正しく動作しない場合があります。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab上では以下を実行してください\n",
    "#!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "#!pip install gym pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab上では以下を実行してください\n",
    "#from pyvirtualdisplay import Display\n",
    "#pydisplay = Display(visible=0, size=(400, 300))\n",
    "#pydisplay.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 DQNでCartpole \\[[link\\]](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習のトイタスクとしてよく利用されるのがCartpoleです。 台車に棒が縦に乗った状態で始まり、 倒れようとする棒を台車を左右に動かすことで倒れないように保つゲームです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make('CartPole-v0') # ゲーム環境の構築\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        env.render() # ゲーム画面の出力\n",
    "\n",
    "        action = env.action_space.sample() # 行動をランダムに選択\n",
    "        next_state, reward, terminal, _ = env.step(action) # 行動を実行し、 次の状態、 報酬、 終端か否かの情報を取得\n",
    "        \n",
    "        ######Colab上で画面を出力するには以下を実行してください\n",
    "        #screen = env.render(mode='rgb_array')\n",
    "\n",
    "        #plt.imshow(screen)\n",
    "        #display.display(plt.gcf())\n",
    "        #display.clear_output(wait=True)\n",
    "        ######\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態、 行動などは以下のようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state: サイズ`(4,)`の`np.ndarary`\n",
    "    - `(カートの位置, カートの速度, ポールの角度, ポールの角速度)`\n",
    "- action\n",
    "    - 0: カートを左に移動させる\n",
    "    - 1: カートを右に移動させる\n",
    "- reward\n",
    "    - (常に) 1.0\n",
    "- terminal\n",
    "    - False: エピソード継続\n",
    "    - True: エピソード終了（ポールが倒れた）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab上では以下を実行してください\n",
    "#pydisplay.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 各構成要素の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状態は4次元ベクトル、 行動の候補数は2つなので、 4->10->2ユニットをもつMLPを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(10, activation='relu', input_shape=(4,)))\n",
    "    model.add(Dense(2))\n",
    "    \n",
    "    model.compile(RMSprop(), 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replayを実現するために、 エージェントが経験した履歴を保存するクラスを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        self.memory = self.memory[-self.memory_size:]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size).tolist()\n",
    "\n",
    "        state      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
    "        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
    "        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
    "        action     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
    "        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])\n",
    "        \n",
    "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Networkを実現するために、 ネットワークの重みをコピーする関数を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(model_original, model_target):\n",
    "    for i, layer in enumerate(model_original.layers):\n",
    "        model_target.layers[i].set_weights(layer.get_weights())\n",
    "    return model_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Replay Memoryへの貯蓄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の初めはReplay Memoryが空で思い出すものが何もないため、 学習を始める前にランダムに行動した履歴をメモリに事前にためておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 10**6 # 10**6\n",
    "initial_memory_size = 5000 # 50000\n",
    "n_actions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "replay_memory = ReplayMemory(memory_size)\n",
    "\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        action = np.random.randint(0, n_actions)\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        reward = np.sign(reward)\n",
    "        \n",
    "        transition = {\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if (step + 1) % 10000 == 0:\n",
    "            print('Number of frames:', step + 1)\n",
    "    \n",
    "    if step >= initial_memory_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_mlp()\n",
    "model_target = build_mlp()\n",
    "\n",
    "eps_start = 1.0\n",
    "eps_end = 0.1\n",
    "n_steps = 10000\n",
    "\n",
    "gamma = 0.99\n",
    "target_update_interval = 1000\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_eps(step):\n",
    "    return max(0.1, (eps_end - eps_start)/n_steps * step + eps_start)\n",
    "\n",
    "n_episodes = 500\n",
    "\n",
    "def create_target(y, _t, action, n_actions):\n",
    "    one_hot = to_categorical(action, n_actions)\n",
    "    t = (1 - one_hot) * y + one_hot * _t[:, None]\n",
    "    \n",
    "    return t\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size):\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "    \n",
    "    try:\n",
    "        q = model.predict(batch['state'])\n",
    "    except:\n",
    "        from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    q_target_next = model_target.predict(batch['next_state'])\n",
    "    \n",
    "    _t = batch['reward'] + (1 - batch['terminal']) * gamma * q_target_next.max(1)\n",
    "    t = create_target(q, _t, batch['action'], n_actions)\n",
    "    \n",
    "    return model.fit(batch['state'], t, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_q_max = []\n",
    "    while not terminal:\n",
    "        q = model.predict(state[None]).flatten()\n",
    "        total_q_max.append(np.max(q))\n",
    "        \n",
    "        eps = get_eps(step)\n",
    "        if np.random.random() < eps:\n",
    "            action = np.random.randint(0, n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q)\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        reward = np.sign(reward)\n",
    "        total_reward += reward\n",
    "        \n",
    "        transition = {\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition)\n",
    "        \n",
    "        train(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if (step + 1) % target_update_interval == 0:\n",
    "            model_target = copy_weights(model, model_target)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}'.format(episode + 1, total_reward, np.mean(total_q_max), eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたモデルをテストしてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    state = env.reset()\n",
    "    terminal = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    while not terminal:\n",
    "        #env.render()\n",
    "        \n",
    "        q = model.predict(state[None]).flatten()\n",
    "        action = np.argmax(q)\n",
    "        \n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
