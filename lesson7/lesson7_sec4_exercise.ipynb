{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 ニューラルネットでゲームを攻略するAIをつくろう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Section 1 解説\n",
    "    - 1.1 強化学習とは\n",
    "    - 1.2 強化学習の定式化\n",
    "    - 1.3 Temporal Difference (TD) 学習\n",
    "    - 1.4 Q学習\n",
    "    - 1.5 Deep Q学習\n",
    "    - 1.6 Checkクイズ\n",
    "- Section 2 実装①\n",
    "    - 2.1 OpenAI Gymについて\n",
    "    - 2.2 Coalb上でのレンダリングについて\n",
    "    - 2.3 DQNでCartPole\n",
    "- Section 3 テクニック・発展的内容\n",
    "    - 3.1 Prioritized Experience Replay\n",
    "    - 3.2 Double DQN\n",
    "    - 3.3 Dueling Network\n",
    "    - 3.4 Checkクイズ\n",
    "- Section 4 実装②\n",
    "    - 4.1 DQN + テクニックでBreakout\n",
    "- Section 5 ケーススタディ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkクイズの解答\n",
    "問1: ③, 問2: ①"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 実装②"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このSectionでは、Section3で紹介したテクニック\n",
    "\n",
    "- Prioritized Experience Replay\n",
    "- Double Deep Q-Network\n",
    "- Dueiling Network\n",
    "\n",
    "を使ってAtariのゲームの一つであるBreakout (ブロック崩し) をプレイ・攻略してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Input, Conv2D, MaxPooling2D, Flatten, Lambda, Add\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. DQN + テクニックでBreakout (ブロック崩し)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakout\n",
    "- state: サイズ`(210, 160, 3)`の`np.ndarary`\n",
    "- action\n",
    "    - 0: 動かない\n",
    "    - 1: 新しい玉を発射\n",
    "    - 2: 右に移動\n",
    "    - 3: 左に移動\n",
    "- reward\n",
    "    - 0.0: ブロックを崩さなかった\n",
    "    - 1.0: ブロックを崩した\n",
    "- terminal\n",
    "    - False: エピソード継続\n",
    "    - True: エピソード終了（5機失うと終わり）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 各構成要素の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのままの画像ではサイズが大きすぎるので、画像を縮小した上でグレースケール化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs, last_obs):\n",
    "    processed_obs = np.maximum(obs, last_obs)\n",
    "    processed_obs = rgb2gray(processed_obs)*255# rgb to gray\n",
    "    processed_obs = resize(processed_obs, (84,84), mode='reflect').astype(np.uint8) # 84x84\n",
    "    return processed_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフ性を満たすために、連続する4つのフレームをstackしてネットワークに入力します。\n",
    "したがって、入力次元は84x84x4となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state(next_obs, obs):\n",
    "    processed_obs = preprocess(next_obs, obs)\n",
    "    state = np.stack([processed_obs for _ in range(4)], axis=2)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(state, next_obs):\n",
    "    state = np.concatenate([state[:, :, 1:], next_obs[:, :, None]], axis=2)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ネットワークにはCNNを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "def huber_loss(y_true, y_pred):\n",
    "    error = K.abs(y_pred - y_true)\n",
    "    quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "    linear_part = error - quadratic_part\n",
    "    loss = K.sum(0.5*K.square(quadratic_part) + linear_part)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    inputs = Input(shape=(84, 84, 4))\n",
    "\n",
    "    # 84x84x4 -> 20x20x32\n",
    "    z = Conv2D(32, kernel_size=(8, 8), strides=(4, 4), activation='relu')(inputs)\n",
    "    # 20x20x32 -> 9x9x64\n",
    "    z = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), activation='relu')(z)\n",
    "    # 9x9x64 ->  7x7x64\n",
    "    z = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu')(z)\n",
    "    #  7x7x64 -> 3136\n",
    "    z = Flatten()(z)\n",
    "    \n",
    "    # Dueling Architecture --------\n",
    "    # State value\n",
    "    z_s = Dense(512, activation='relu')(z)\n",
    "    z_s = Dense(1)(z_s)\n",
    "    # Advantage value\n",
    "    z_a = Dense(512, activation='relu')(z)\n",
    "    z_a = Dense(4)(z_a)\n",
    "    z_a = Lambda(lambda x: x - K.stop_gradient(K.mean(x, axis=1, keepdims=True)))(z_a)\n",
    "    \n",
    "    y = Add()([z_s, z_a])\n",
    "\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    model.compile(RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), huber_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized Experience Replayを実現するために、 各履歴におけるTD誤差をもとにサンプリングを行うクラスを実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        self.memory = self.memory[-self.memory_size:]\n",
    "        \n",
    "    def make_prob(self, td_errors, alpha=0.5, eps=np.float32(0.001)):\n",
    "        abs_td_errors = np.power(np.abs(td_errors) + eps, alpha)\n",
    "        return abs_td_errors / np.sum(abs_td_errors)\n",
    "    \n",
    "    def td_error_update(self, batch_indexes):\n",
    "        for i in batch_indexes:\n",
    "            q = model.predict(self.memory[i]['state'][None] / 255.).flatten()\n",
    "            new_q = model_target.predict(self.memory[i]['next_state'][None] / 255.).flatten()\n",
    "            self.memory[i]['td_error'] = self.memory[i]['reward'] + (1-self.memory[i]['terminal']) * gamma * np.max(new_q) - q[self.memory[i]['action']]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        td_errors = [self.memory[index]['td_error'] for index in range(len(self.memory))]\n",
    "        #抽出確率の計算\n",
    "        p = self.make_prob(td_errors)\n",
    "        #抽出確率を元にサンプリングする経験を決定\n",
    "        batch_indexes = np.random.choice(len(self.memory), size=batch_size, p=p).tolist()\n",
    "        #TD誤差の更新\n",
    "        self.td_error_update(batch_indexes)\n",
    "        #経験のサンプリング\n",
    "        state      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
    "        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
    "        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
    "        action     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
    "        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])\n",
    "        \n",
    "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(model_original, model_target):\n",
    "    for i, layer in enumerate(model_original.layers):\n",
    "        model_target.layers[i].set_weights(layer.get_weights())\n",
    "    return model_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Replay Memoryへの経験の貯蓄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartpoleと同様、 あらかじめMemoryに経験をためておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 200000\n",
    "initial_memory_size = 20000\n",
    "n_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "replay_memory = PrioritizedReplayMemory(memory_size)\n",
    "\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    next_obs, _, _, _ = env.step(0)\n",
    "    \n",
    "    next_state = get_initial_state(next_obs, obs)\n",
    "    terminal = False\n",
    "    \n",
    "    while not terminal:\n",
    "        obs = next_obs\n",
    "        state = next_state\n",
    "        \n",
    "        action = np.random.randint(0, n_actions)\n",
    "        \n",
    "        next_obs, reward, terminal, _ = env.step(action)\n",
    "        reward = np.sign(reward)\n",
    "        \n",
    "        processed_obs = preprocess(next_obs, obs)\n",
    "        \n",
    "        next_state = get_state(state, processed_obs)\n",
    "        \n",
    "        transition = {\n",
    "            'td_error': reward,\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print('Number of frames:', step + 1)\n",
    "    \n",
    "    if step >= initial_memory_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNの学習には非常に時間がかかるので注意してください。(数十時間~数日)\n",
    "\n",
    "ネットワークの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn()\n",
    "model_target = build_cnn()\n",
    "\n",
    "eps_start = 1.0\n",
    "eps_end = 0.1\n",
    "n_steps = 500000\n",
    "\n",
    "gamma = 0.99\n",
    "target_update_interval = 10**4\n",
    "n_noop_steps = 30\n",
    "train_interval = 4\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def get_eps(step):\n",
    "    return max(0.1, (eps_end - eps_start)/n_steps * step + eps_start)\n",
    "\n",
    "n_episodes = 3000 # 学習を行うエピソード数\n",
    "\n",
    "def create_target(y, _t, action, n_actions):\n",
    "    one_hot = to_categorical(action, n_actions)\n",
    "    t = (1 - one_hot) * y + one_hot * _t[:, None]\n",
    "    \n",
    "    return t\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size):\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "    \n",
    "    q = model.predict(batch['state'] / 255.)\n",
    "    q_next = model.predict(batch['next_state'] / 255.) \n",
    "    q_target_next = model_target.predict(batch['next_state'] / 255.)\n",
    "    \n",
    "    a_next = q_next.argmax(1) # (batch_size,)\n",
    "    a_next_onehot = to_categorical(a_next, n_actions) # (batch_size, n_actions)\n",
    "    \n",
    "    _t = batch['reward'] + (1 - batch['terminal']) * gamma * (q_target_next * a_next_onehot).sum(1)\n",
    "    t = create_target(q, _t, batch['action'], n_actions)\n",
    "    \n",
    "    return model.fit(batch['state'] / 255., t, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_episodesの数だけ学習を行います。（学習済みの重みを試す場合は飛ばしてください。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    next_obs, _, terminal, _ = env.step(0)\n",
    "    for _ in range(np.random.randint(n_noop_steps)):\n",
    "        obs = next_obs\n",
    "        next_obs, _, _, _ = env.step(0)\n",
    "    next_state = get_initial_state(next_obs, obs)\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_q_max = []\n",
    "    while not terminal:\n",
    "        obs = next_obs\n",
    "        state = next_state\n",
    "        \n",
    "        q = model.predict(state[None] / 255.).flatten()\n",
    "        total_q_max.append(np.max(q))\n",
    "        \n",
    "        eps = get_eps(step)\n",
    "        if np.random.random() < eps:\n",
    "            action = np.random.randint(0, n_actions)\n",
    "        else:\n",
    "            action = np.argmax(q)\n",
    "        next_obs, reward, terminal, _ = env.step(action)\n",
    "        processed_obs = preprocess(next_obs, obs)\n",
    "        reward = np.sign(reward)\n",
    "        total_reward += reward\n",
    "        next_state = get_state(state, processed_obs)\n",
    "        \n",
    "        #TD誤差の計算\n",
    "        new_q = model_target.predict(next_state[None] / 255.).flatten()\n",
    "        td_error = reward + (1-terminal) * gamma * np.max(new_q) - q[action]\n",
    "        \n",
    "        transition = {\n",
    "            'td_error': td_error,\n",
    "            'state': state,\n",
    "            'next_state': next_state,\n",
    "            'reward': reward,\n",
    "            'action': action,\n",
    "            'terminal': int(terminal)\n",
    "        }\n",
    "        replay_memory.append(transition)\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        if (step + 1) % train_interval == 0:\n",
    "            train(batch_size)\n",
    "\n",
    "        if (step + 1) % target_update_interval == 0:\n",
    "            model_target = copy_weights(model, model_target)\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    if (episode + 1) % 10 == 0:\n",
    "        print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}'.format(episode + 1, total_reward, np.mean(total_q_max), eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたモデルをテストしてみます。（40時間ほど学習させた重みを利用する場合はLOAD_WEIGHTSをTrueに変更してください.）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習済みの重みを利用するかどうか\n",
    "LOAD_WEIGHTS = False\n",
    "if LOAD_WEIGHTS:\n",
    "    model.load_weights('./data/param.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Colab上で画面を出力するには以下を実行してください\n",
    "#%matplotlib inline\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "#!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "\n",
    "#from pyvirtualdisplay import Display\n",
    "#pydisplay = Display(visible=0, size=(400, 300))\n",
    "#pydisplay.start()\n",
    "######\n",
    "\n",
    "def test(max_steps=5000):\n",
    "    obs = env.reset()\n",
    "    next_obs, _, terminal, _ = env.step(0)\n",
    "    next_state = get_initial_state(next_obs, obs)\n",
    "    \n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    while not terminal:\n",
    "        obs = next_obs\n",
    "        state = next_state\n",
    "        \n",
    "        q = model.predict(state[None]/255.).flatten()\n",
    "        action = np.argmax(q)\n",
    "        \n",
    "        next_obs, reward, terminal, _ = env.step(action)\n",
    "        processed_obs = preprocess(next_obs, obs)\n",
    "        total_reward += reward\n",
    "        next_state = get_state(state, processed_obs)\n",
    "        ######Colab上で画面を出力するには以下をコメントアウトしてください\n",
    "        env.render()\n",
    "        ######\n",
    "        step += 1\n",
    "        \n",
    "        ######Colab上で画面を出力するには以下を実行してください\n",
    "        #screen = env.render(mode='rgb_array')\n",
    "\n",
    "        #plt.imshow(screen)\n",
    "        #display.display(plt.gcf())\n",
    "        #display.clear_output(wait=True)\n",
    "        ######\n",
    "        \n",
    "        if step > max_steps:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
