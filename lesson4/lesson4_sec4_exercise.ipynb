{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ti6es8PG5xe"
   },
   "source": [
    "# Lesson4 ニューラル翻訳モデルを作ってみよう（Seq2Seq, Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FyQM1pEG5xj"
   },
   "source": [
    "## 目次\n",
    "\n",
    "- Section3 Checkクイズの解答\n",
    "- Section4 実装②\n",
    "    - 4.0 データの用意\n",
    "    - 4.1 モデル構築\n",
    "    - 4.2 モデルの学習\n",
    "    - 4.3 モデルによる予測\n",
    "    - 4.4 モデルの可視化\n",
    "- ケーススタディ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGH8KiGFG5xk"
   },
   "source": [
    "## Section3 Checkクイズの解答\n",
    "\n",
    "問題1: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mBwoLrUG5xl"
   },
   "source": [
    "## Section4 実装②"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqVY65W_G5xo"
   },
   "source": [
    "先程のLSTMを使ったSeq2SeqモデルにAttention機構を導入したうえで英日機械翻訳を行ってみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czs8DRhfG5xp"
   },
   "source": [
    "### 4.0 データの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uapUh1xuG5xq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):\n",
    "    tokenizer = Tokenizer(filters=\"\")\n",
    "    whole_texts = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \" + line.strip() + \" </s>\")\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts), tokenizer\n",
    "\n",
    "# 読み込み＆Tokenizerによる数値化\n",
    "x_train, tokenizer_en = load_data('data/train.en')\n",
    "y_train, tokenizer_ja = load_data('data/train.ja')\n",
    "\n",
    "en_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "ja_vocab_size = len(tokenizer_ja.word_index) + 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.02, random_state=42)\n",
    "\n",
    "# パディング\n",
    "x_train = pad_sequences(x_train, padding='post')\n",
    "y_train = pad_sequences(y_train, padding='post')\n",
    "\n",
    "seqX_len = len(x_train[0])\n",
    "seqY_len = len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7f5Vmmp1G5xv"
   },
   "source": [
    "### 4.1 モデル構築\n",
    "\n",
    "モデルにAttention機構を導入します。\n",
    "以下の図のようにDecoder（復号化器）の出力$h_t$に対して、　Encoder（符号化期）の状態を考慮した$\\tilde{h_t}$を出力させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qroJCPoqG5xv"
   },
   "source": [
    "<img src='figures/attention_model.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQgZM15HG5xx"
   },
   "source": [
    "Attention機構としてattention $a_t$及び文脈ベクトル$c_t$を用意することに注意して実装してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxomsYUqG5xy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Permute, Activation, Embedding, Dense, LSTM, concatenate, dot\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 256\n",
    "att_dim = 256\n",
    "\n",
    "# 符号化器\n",
    "encoder_inputs = Input(shape=(seqX_len,))\n",
    "encoder_embedded = Embedding(en_vocab_size, emb_dim, mask_zero=True)(encoder_inputs)\n",
    "encoded_seq, *encoder_states = LSTM(hid_dim, return_sequences=True, return_state=True)(encoder_embedded)\n",
    "\n",
    "# 復号化器（encoder_statesを初期状態として指定）\n",
    "decoder_inputs = Input(shape=(seqY_len,))\n",
    "decoder_embedding = Embedding(ja_vocab_size, emb_dim)\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(hid_dim, return_sequences=True, return_state=True)\n",
    "decoded_seq, _, _ = decoder_lstm(decoder_embedded, initial_state=encoder_states)\n",
    "\n",
    "# Attention\n",
    "score_dense = Dense(hid_dim)\n",
    "score = score_dense(decoded_seq)                        # shape: (seqY_len, hid_dim) -> (seqY_len, hid_dim)\n",
    "score = dot([score, encoded_seq], axes=(2,2))           # shape: [(seqY_len, hid_dim), (seqX_len, hid_dim)] -> (seqY_len, seqX_len)\n",
    "attention = Activation('softmax')(score)                # shape: (seqY_len, seqX_len) -> (seqY_len, seqX_len)\n",
    "context = dot([attention, encoded_seq], axes=(2,1))     # shape: [(seqY_len, seqX_len), (seqX_len, hid_dim)] -> (seqY_len, hid_dim)\n",
    "concat = concatenate([context, decoded_seq], axis=2)    # shape: [(seqY_len, hid_dim), (seqY_len, hid_dim)] -> (seqY_len, 2*hid_dim)\n",
    "attention_dense = Dense(att_dim, activation='tanh')\n",
    "attentional = attention_dense(concat)                   # shape: (seqY_len, 2*hid_dim) -> (seqY_len, att_dim)\n",
    "output_dense = Dense(ja_vocab_size, activation='softmax')\n",
    "outputs = output_dense(attentional)                     # shape: (seqY_len, att_dim) -> (seqY_len, ja_vocab_size)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQN2-_ATG5x2"
   },
   "source": [
    "### 4.2 モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYFdfWrFG5x2"
   },
   "source": [
    "section2と同様にモデルの学習を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn_8qSAmG5x4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target = np.hstack((y_train[:, 1:], np.zeros((len(y_train),1), dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train, y_train], np.expand_dims(train_target, -1), batch_size=128, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VvE2pUUG5x7"
   },
   "source": [
    "### 4.3 モデルによる生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9VqA1-0G5x8"
   },
   "source": [
    "生成についてもsection2と同じような仕組みですが、Attentionのモデルで学習していることに注意が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxx18EVMG5x9"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoded_seq]+encoder_states)\n",
    "\n",
    "decoder_states_inputs = [Input(shape=(hid_dim,)), Input(shape=(hid_dim,))]\n",
    "\n",
    "decoder_inputs = Input(shape=(1,))\n",
    "decoder_embedded = decoder_embedding(decoder_inputs)\n",
    "decoded_seq, *decoder_states = decoder_lstm(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoded_seq] + decoder_states)\n",
    "\n",
    "# Attention\n",
    "encoded_seq_in, decoded_seq_in = Input(shape=(seqX_len, hid_dim)), Input(shape=(1, hid_dim))\n",
    "score = score_dense(decoded_seq_in)\n",
    "score = dot([score, encoded_seq_in], axes=(2,2))\n",
    "attention = Activation('softmax')(score)\n",
    "context = dot([attention, encoded_seq_in], axes=(2,1))\n",
    "concat = concatenate([context, decoded_seq_in], axis=2)\n",
    "attentional = attention_dense(concat)\n",
    "attention_outputs = output_dense(attentional)\n",
    "\n",
    "attention_model = Model([encoded_seq_in, decoded_seq_in], [attention_outputs, attention])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lTjn2Q3G5yC"
   },
   "source": [
    "このモデルを使用した生成（予測）を行ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJx-_URIG5yD"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, bos_eos, max_output_length = 1000):\n",
    "    encoded_seq, *states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array(bos_eos[0])  # bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    output_seq = bos_eos[0][:]\n",
    "    attention_seq = np.empty((0,len(input_seq[0])))\n",
    "    \n",
    "    while True:\n",
    "        decoded_seq, *states_value = decoder_model.predict([target_seq] + states_value)\n",
    "        output_tokens, attention = attention_model.predict([encoded_seq, decoded_seq])\n",
    "        sampled_token_index = [np.argmax(output_tokens[0, -1, :])]\n",
    "        output_seq += sampled_token_index\n",
    "        attention_seq = np.append(attention_seq, attention[0], axis=0)\n",
    "        \n",
    "        if (sampled_token_index == bos_eos[1] or len(output_seq) > max_output_length):\n",
    "            break\n",
    "\n",
    "        target_seq = np.array(sampled_token_index)\n",
    "\n",
    "    return output_seq, attention_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Het5ndGGG5yI"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "detokenizer_en = dict(map(reversed, tokenizer_en.word_index.items()))\n",
    "detokenizer_ja = dict(map(reversed, tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no = 0\n",
    "input_seq = pad_sequences([x_test[text_no]], seqX_len, padding='post')\n",
    "bos_eos = tokenizer_ja.texts_to_sequences([\"<s>\", \"</s>\"])\n",
    "\n",
    "output_seq, attention_seq = decode_sequence(input_seq, bos_eos)\n",
    "\n",
    "print('元の文:', ' '.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文:', ' '.join([detokenizer_ja[i] for i in output_seq]))\n",
    "print('正解文:', ' '.join([detokenizer_ja[i] for i in y_test[text_no]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gHMWLk9G5yQ"
   },
   "source": [
    "ここではさらに、Attentionの分布も描画してみましょう。（環境によっては、縦軸の日本語ラベルが文字化けするかもしれません）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bj2sBtaoG5yR"
   },
   "outputs": [],
   "source": [
    "# 描画\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.pcolor(attention_seq[:,:len(x_test[text_no])], cmap=plt.cm.Blues, vmax=1)\n",
    "ax.set_xticks(np.arange(len(x_test[text_no])) + 0.5, minor=False)\n",
    "ax.set_yticks(np.arange(attention_seq.shape[0]) + 0.5, minor=False)\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticklabels([detokenizer_en[i] for i in x_test[text_no]], minor=False)\n",
    "ax.set_yticklabels([detokenizer_ja[i] for i in output_seq[1:]], minor=False)\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9N44pHKgG5yX"
   },
   "source": [
    "### 4.4 モデルの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhjfFxioG5yY"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lesson4_sec4_exercise_master.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
